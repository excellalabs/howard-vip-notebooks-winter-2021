{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pytorch_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa475668",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db119269",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # or any other checkpoint\n",
    "word_embeddings = model.transformer.wte.weight  # Word Token Embeddings \n",
    "position_embeddings = model.transformer.wpe.weight  # Word Position Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_transformers as pt \n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import pandas as pd\n",
    "text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('s3://howardvip-clinical-data/clean_noteevents_text.csv',nrows=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ddfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for passage in df['0']:\n",
    "    if \"Discharge Date:\" not in passage:\n",
    "        print(passage)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pt.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = pt.GPT2Model.from_pretrained('gpt2')\n",
    "zz = tokenizer.tokenize(text)\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1=torch.tensor([tokenizer.convert_tokens_to_ids(zz)])\n",
    "output,hidden=model(z1)\n",
    "#ouput.shape\n",
    "#torch.Size([1, 74, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1629478",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The max model length is {} for this model, although the actual embedding size for GPT small is 768\".format(tokenizer.model_max_length))\n",
    "print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1786aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.input_ids = []\n",
    "    self.attn_masks = []\n",
    "\n",
    "    for txt in txt_list:\n",
    "\n",
    "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.attn_masks[idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_notebooks_events_scrape_2500_clean.csv', 'rb') as pickle_file:\n",
    "    textClean = pd.read_csv(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68171434",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GPT2Dataset(textClean, tokenizer, max_length=768)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50237d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.session.Session().default_bucket())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b2e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
