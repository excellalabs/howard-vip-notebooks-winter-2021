{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffdde3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install contractions\n",
    "#pip install fasttext\n",
    "#pip install tqdm\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import fasttext\n",
    "import contractions\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9761e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=70)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e88f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.read_csv('Howard-Local-NOTEEVENTS.csv.gz',nrows=5000)\n",
    "from tqdm import tqdm\n",
    "chunksize = 50000\n",
    "count = 0\n",
    "fields = ['CATEGORY','TEXT']\n",
    "listOFChunks = []\n",
    "for chunk in pd.read_csv(\"Howard-Local-NOTEEVENTS.csv.gz\", chunksize=chunksize,usecols=fields):\n",
    "    #test.append(chunk)\n",
    "    for report in tqdm(range(len(chunk['TEXT']))):\n",
    "        try:\n",
    "            chunk['TEXT'][report+count] = chunk['TEXT'][report+count].replace(\"[**\", \"\").replace(\"**]\", \"\")\n",
    "        except:\n",
    "            None\n",
    "        #test['TEXT'] = test['TEXT'].replace(\"[**\", \"\").replace(\"**]\", \"\")\n",
    "    count += chunksize\n",
    "    listOFChunks.append(chunk)\n",
    "    \n",
    "test = pd.concat(listOFChunks)\n",
    "#test = pd.read_csv('s3://howardvip-clinical-data/NOTEEVENTS.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from tqdm import tqdm\n",
    "for report in tqdm(range(len(test['TEXT']))):\n",
    "    test['TEXT'][report] = test['TEXT'][report].replace(\"[**\", \"\").replace(\"**]\", \"\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe7f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "test.drop('ROW_ID', axis=1, inplace=True)\n",
    "test.drop('SUBJECT_ID', axis=1, inplace=True)\n",
    "test.drop('HADM_ID', axis=1, inplace=True)\n",
    "test.drop('CHARTDATE', axis=1, inplace=True)\n",
    "test.drop('STORETIME', axis=1, inplace=True)\n",
    "test.drop('DESCRIPTION', axis=1, inplace=True)\n",
    "test.drop('CGID', axis=1, inplace=True)\n",
    "test.drop('ISERROR', axis=1, inplace=True)\n",
    "test.drop('CHARTTIME', axis=1, inplace=True)\n",
    "\"\"\"\n",
    "textClean = test.loc[:, ['CATEGORY', 'TEXT']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f51f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "textClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test.columns:\n",
    "    print(col, test[col].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "textClean['text_report'] = textClean['TEXT'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42212378",
   "metadata": {},
   "outputs": [],
   "source": [
    "textClean['text_report_str'] = [' '.join(map(str, l)) for l in textClean['text_report']]\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e467f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "textClean['tokenized'] = textClean['text_report_str'].apply(word_tokenize)\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf34999",
   "metadata": {},
   "outputs": [],
   "source": [
    "textClean['lower'] = textClean['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "punc = string.punctuation\n",
    "textClean['no_punc'] = textClean['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1711ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "textClean['stopwords_removed'] = textClean['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd101b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "textClean['pos_tags'] = textClean['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab93bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "textClean['wordnet_pos'] = textClean['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "textClean['lemmatized'] = textClean['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "textClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textClean['lemmatized'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "textClean.to_csv('ALL_Brackets_notebooks_events_scrape_all_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a4c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "bytes_to_write = df.to_csv(None).encode()\n",
    "fs = s3fs.S3FileSystem(key=key, secret=secret)\n",
    "with fs.open('s3://howardp-cleaned-parsed-data/ALL_Brackets_notebooks_events_scrape_all_clean.csv', 'wb') as f:\n",
    "    f.write(bytes_to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
