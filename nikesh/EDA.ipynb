{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install wordcloud\n",
    "!pip3 install tqdm\n",
    "!pip3 install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import sklearn\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from pyLDAvis import sklearn\n",
    "import pyLDAvis\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NOTEEVENTS.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whole_dataset(func):\n",
    "    def inner():\n",
    "        ret = []\n",
    "        chunks=pd.read_csv(\"NOTEEVENTS.csv.gz\",chunksize=100000)\n",
    "        for chunk in tqdm(chunks):\n",
    "            chunk.columns = [c.lower() for c in chunk.columns]\n",
    "            ret.append(func(chunk))\n",
    "        return ret\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "@whole_dataset\n",
    "def check_na(chunk):\n",
    "    return chunk.text.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-compact",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_na()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(text):\n",
    "    w = text.split()\n",
    "    for word in w:\n",
    "        if word not in STOPWORDS:\n",
    "            words[word.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "@whole_dataset\n",
    "def get_word_counts(chunk):\n",
    "    chunk.text.apply(lambda x:count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=800, height=600,background_color=\"white\", max_words=1000)\n",
    "# generate word cloud\n",
    "wc.generate_from_frequencies(words)\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections =defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-therapist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_count(text):\n",
    "    matches = [i[0] for i in  re.findall('\\n((.?)*):\\n',text)]\n",
    "    for match in matches:\n",
    "        sections[match.lower().strip()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-diesel",
   "metadata": {},
   "outputs": [],
   "source": [
    "@whole_dataset\n",
    "def get_section_counts(chunk):\n",
    "    chunk.text.apply(lambda x:section_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_section_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections['assessment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sections, key = lambda x: sections[x], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "des= sorted(sections, key = lambda x: sections[x], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "des =[s.strip() for s in des ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample =des[:30]\n",
    "sample_count = [sections[i] for i in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(sample,sample_count)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.tokenize.sent_tokenize(chunk.TEXT[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer.fit(chunkgen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = chunkgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkgen():\n",
    "    chunks=pd.read_csv(\"NOTEEVENTS.csv.gz\",chunksize=500000)\n",
    "    for chunk in tqdm(chunks):\n",
    "        chunk.columns = [c.lower() for c in chunk.columns]\n",
    "        text  = chunk.text.to_numpy()\n",
    "        del chunk\n",
    "        for i in text:\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_features = 100\n",
    "\n",
    "# LDA uses raw term frequencies\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=nbr_features, stop_words='english')\n",
    "term_freqs = tf_vectorizer.fit_transform(df.TEXT)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "# Plot pretty LDA output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-orientation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_topics = 20\n",
    "lda = LatentDirichletAllocation(n_components=nbr_topics,verbose=1, max_iter=5, learning_offset=50.,random_state=3).fit(term_freqs)\n",
    "lda_topics = lda.transform(term_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vis_data = pyLDAvis.sklearn.prepare(lda, term_freqs, tf_vectorizer)\n",
    "lda_vis_data_html = pyLDAvis.prepared_data_to_html(lda_vis_data)\n",
    "pyLDAvis.display(lda_vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-casting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
